---
title: "Social Equity, Access and the New York School System"
author: "Kunal Kerai"
date: "August 29, 2018"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background and Problem Statement
PASSNYC -- a not-for-profit organization that facilitates a collective impact-- is dedicated to broadening educational opportunities for New York City's talented and underserved students. New York City is home to some of the most impressive educational institutions in the world, yet in recent years, the City's specialized high schools - institutions with historically transformative impact on student outcomes - have seen a shift toward more homogeneous student body demographics.

PASSNYC uses public data to identify students within New York City's under-performing school districts and, through consulting and collaboration with partners, aims to increase the diversity of students taking the Specialized High School Admissions Test (SHSAT). By focusing efforts in under-performing areas that are historically underrepresented in SHSAT registration, we will help pave the path to specialized high schools for a more diverse group of students.

With limited time and resources, PASSNYC must be strategic in systemically improving the diversity pipeline and social mobility of certain disadvantaged groups into the specialized schools. The main question to be answered is where will PASSNYC's investment produce the greatest ROI for the services they offer (after school programs, test preparation, mentoring, or resources for parents)?

## Data Dictionary

* school: school name	
* DBN: State Education Department Code	
* district	
* lat: lattitude coordinates of school	
* long: longitude coordinates of school 	
* address: full address of school	
* City	
* zip	
* commSchool: Is the school a community school (Y/N)?
* eni	income: (%temp housing) + (% HRA eligible x 0.5) + (% free lunch eligible x 0.5). The higher the index, the higher the need.
* pctELL: percent of English Language Learners


* Race Percentages: percent of school that identifies as a certain race. 
    + pctAsian
    + pctBlack
    + pctHispanic
    + pctBlackHispanic:  percent of Black students + percent of Hispanic students
    + pctWhite: White
    
  
* Attendance
    + pctAttend: total number of days attended by all students / total number of days on register for all students	
    + pctAbsentChronic: percent of students missing 10% of school days - or 18 days+ per year in a 180-day school year


* Psychosocial Ratings
    + pctRigor: How well the curriculum and instruction engage students, build critical-thinking skills, and are aligned to the Common Core
    + ratingRigor: How well the curriculum and instruction engage students, build critical-thinking skills, and are aligned to the Common Core
    + pctCollab: How well teachers participate in opportunities to develop, grow, and contribute to the continuous improvement of the school community  
    + ratingCollab: How well teachers participate in opportunities to develop, grow, and contribute to the continuous improvement of the school community
    + pctSupp: How well the school establishes a culture where students feel safe, challenged to grow, and supported to meet high expectations
    + ratingSupp: How well the school establishes a culture where students feel safe, challenged to grow, and supported to meet high expectations 	
    + pctLeader: How well school leadership inspires the school community with a clear instructional vision and effectively distributes leadership to realize this vision	
    + ratingLeader: How well school leadership inspires the school community with a clear instructional vision and effectively distributes leadership to realize this vision	
    + pctCommunity: How well the school forms effective partnerships with families to improve the school	
    + ratingCommunity: How well the school forms effective partnerships with families to improve the school
    + pctTrust: Whether the relationships between administrators, educators, students, and families are based on trust and respect
    + ratingTrust: Whether the relationships between administrators, educators, students, and families are based on trust and respect

* Average School Scores for English (ELA) and Math sections
    + avgELA
    + avgMath
    + academicScore (sum of ELA and Math)

* Students and test statistics for District 5/6
    + enroll: number of students enrolled in the school
    + registered: number of students who registered for SHSAT
    + took: number of students who took the SHSAT
    + regPct: registered/enroll
    + tookPct: took/enroll
    + yield: took/registered
    
* Numeric values for psychosocial categories (0= NA, 1= Not Meeting Expectations, 2= Approaching Expectations, 3= Meeting Expectations, 4= Exceeeding Expectations))    
    + quantCollab
    + quantSupp
    + quantRigor
    + quantLeader
    + quantCommunity
    + quantTrust
    
* Number of URM students who received the highest score possible for the English and Math sections by race:
    + elaBlack
    + elaHispanic
    + mathBlack
    + mathHispanic


## Data Cleaning

### Loading Libraries and Data
Let's load some libraries we will use.
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
library(ggplot2)
library(cluster)
library(scatterplot3d)
library(digest)
library(corrplot)
library(ggmap)
library(factoextra)
library(data.table)
library(DT)
library(GGally)
library(caret)
library(cluster)
library(randomForest)
library(mlbench)
library(ggmap)
```


Let's load our data from the table:
```{r message=FALSE, warning=FALSE, include= T , paged.print= F}
reg <- read_csv("D5 SHSAT Registrations and Testers.csv")
school <- read_csv("2016 School Explorer.csv")
```

Before we proceed, let's take a look at the initial data.
```{r message=FALSE, warning=FALSE, include= T , paged.print= F}
head(reg)

head(school)
```

Our data looks pretty clean by most standards, but there is work to be done for sure. For example, we'll need to rename some of our variable names, join our two tables, etc. Let's move forward with some cleaning.
 
In order to make our data analysis easier, let's start cleaning the data.

### Cleaning our Data
We begin by renaming the columns we plan to use.


```{r include=FALSE}

reg<- reg %>%
  rename(school = `School name`,
         year = `Year of SHST`,
         grade = `Grade level`,
         enroll = `Enrollment on 10/31`,
         registered = `Number of students who registered for the SHSAT`,
         took = `Number of students who took the SHSAT`)

#Finding the average number of students at each school, how many registered, and how many took the test
reg<- reg %>%
  group_by(school) %>%
  mutate_if(is.numeric, mean) %>%
  mutate_if(is.character, funs(paste(unique(.), collapse = "_"))) %>%
  distinct()

#Changing the case will let us match and eventually join the tables.
reg$school<-toupper(reg$school)

#Let's remove some columns that have no use for our future analysis.
reg<-reg %>%
  select(-year )%>%
  select(-grade)


#Calculating percentages of those who registered of the total population, registered AND took the test, and those who took the test of the total population
reg<-reg%>%
  mutate(regPct = registered*100/enroll, tookPct = took*100/enroll, yield = took/registered)

#What does the table currently look like?
str(reg)


#Let's clean up the other sheet
school<-read_csv("2016 School Explorer.csv")

#what does the table look like?
str(school)

#renaming some columns
head(school)

school<- school %>%
  rename(school = `School Name`,
         address = `Address (Full)`,
         DBN = `Location Code`, 
         lat =  Latitude,
         long = Longitude,
         zip = Zip,
         eni = `Economic Need Index`,
         district = District,
         commSchool = `Community School?`,
         income = `School Income Estimate`,
         pctELL = `Percent ELL`,
         pctAsian = `Percent Asian`,
         pctBlack = `Percent Black`,
         pctBlackHispanic = `Percent Black / Hispanic`,
         pctHispanic = `Percent Hispanic`,
         pctWhite = `Percent White`,
         pctAttend = `Student Attendance Rate`,
         pctAbsentChronic = `Percent of Students Chronically Absent`,
         pctRigor = `Rigorous Instruction %`,
         ratingRigor = `Rigorous Instruction Rating`,
         pctCollab = `Collaborative Teachers %`,
         ratingCollab = `Collaborative Teachers Rating`,
         pctSupp = `Supportive Environment %`,
         ratingSupp = `Supportive Environment Rating`,
         pctLeader = `Effective School Leadership %`,
         ratingLeader = `Effective School Leadership Rating`,
         pctCommunity = `Strong Family-Community Ties %`,
         ratingCommunity = `Strong Family-Community Ties Rating`,
         pctTrust = `Trust %`,
         ratingTrust = `Trust Rating`,
         avgELA = `Average ELA Proficiency`,
         avgMath = `Average Math Proficiency`,
         mathAll = `Grade 8 Math - All Students Tested`,
         mathAll4 = `Grade 8 Math 4s - All Students`,
         mathBlack = `Grade 8 Math 4s - Black or African American`,
         mathHispanic = `Grade 8 Math 4s - Hispanic or Latino`,
         mathAsian = `Grade 8 Math 4s - Asian or Pacific Islander`,
         mathWhite = `Grade 8 Math 4s - White`,
         elaAll = `Grade 8 ELA - All Students Tested`,
         elaAll4 = `Grade 8 ELA 4s - All Students`,
         elaBlack = `Grade 8 ELA 4s - Black or African American`,
         elaHispanic = `Grade 8 ELA 4s - Hispanic or Latino`,
         elaAsian = `Grade 8 ELA 4s - Asian or Pacific Islander`,
         elaWhite = `Grade 8 ELA 4s - White` )

 
#remove unneccessary columns
school<-school%>%
  select(-`Adjusted Grade`)%>%
  select(-`New?`)%>%
  select(-`Other Location Code in LCGMS`)%>%
  select(-`SED Code`)%>%
  select(-starts_with('Grade'))        

#now we want to join the tables together
school_clean <- full_join(school, reg, by = "DBN" )

#changing some columns to numeric values (as they should be)

school_clean<- school_clean %>% 
  mutate_at(vars(contains('pct')),funs(as.numeric(gsub("%", "", ., fixed = TRUE))/100)) %>%
  mutate(income = as.numeric(gsub('\\$|,', '', income))) %>%
  mutate(eni = as.numeric(eni))%>%
  mutate(avgELA = as.numeric(avgELA),
         avgMath = as.numeric(avgMath),
         academicScore = avgMath + avgELA)



#making some final touches on our new table
school_clean <- school_clean%>%
  filter(!is.na(school.x))%>%
  select(-school.y)%>%
  rename(school = school.x)

#let's turn some qualitative ratings into quantitative scores. First, let's create a function to make creating these new columns easier.
quantScore <- function(x){
  if_else(grepl("Not Meeting Target", x),1 ,
          if_else(grepl("Approaching Target", x),2 ,
                  if_else(grepl("Meeting Target", x),3 , 
                          if_else(grepl("Exceeding Target", x), 4, 0))))
  
}

school_clean<- school_clean%>%
  mutate(quantRigor = quantScore(ratingRigor)) %>%
  mutate(quantCollab = quantScore(ratingCollab)) %>%
  mutate(quantSupp = quantScore(ratingSupp)) %>%
  mutate(quantLeader = quantScore(ratingLeader)) %>%
  mutate(quantCommunity = quantScore(ratingCommunity)) %>%
  mutate(quantTrust = quantScore(ratingTrust))


school_clean <- school_clean%>%
  mutate(pctELA4 = elaAll4 / elaAll,
          pctELABlack = elaBlack / elaAll4,
          pctELAHispanic = elaHispanic / elaAll4,
          pctELAAsian = elaAsian / elaAll4,
          pctELAWhite = elaWhite/ elaAll4,
          pctMath4 = mathAll4 / mathAll4,
          pctMathBlack = mathBlack / mathAll4,
          pctMathHispanic = mathHispanic / mathAll4,
          pctMathAsian = mathAsian / mathAll4,
          pctMathWhite = mathWhite / mathAll4,
          URM4 = mathHispanic + mathBlack + elaHispanic + elaBlack)


school_clean[ school_clean == "NaN"] <- 0

school_clean<- school_clean%>%
  mutate(avgELA= ifelse(is.na(avgELA), mean(avgELA, na.rm=TRUE), avgELA),
         avgMath= ifelse(is.na(avgMath), mean(avgMath, na.rm=TRUE), avgMath),
         academicScore = ifelse(is.na(academicScore), mean(academicScore, na.rm=TRUE), academicScore))

school_clean <- school_clean %>%
      mutate(commSchoolBinary = ifelse(commSchool == "No",0,1))
          

write_csv(school_clean, "SHSAT_data.csv")


str(school_clean)

         
```         

Before moving into our EDA, Hispanic is considered a race by NYC. Student who identify as multiple races are places 

## Exploratory Data Analysis

Before continuing, there are three things we need to consider when analyzing our data:

* PassNYC cares about increasing diversity; therefore need to look at low income, and underrepresented racial minorities (URM) students
* PassNYC wants to improve the situation through their offerings: after school programs, test preparation, mentoring, and resources for parents
* Success will be measured by how many student register then take the test, as well as the academic performance of the school as an indicator.

We consider a school to be economically stratified if its economic need as measured by the Economic
Need Index1 is more than 10 percentage points from the citywide average. A school can be stratified in either direction - by serving more low-income or more high-income children. New York City reports that 70.6% of schools are economically stratified today. 




### Demographics
```{r echo=F, message=FALSE, warning=FALSE, paged.print=TRUE}
reg3<-gather(school_clean, race, percent, pctAsian:pctWhite, factor_key=TRUE)

head(reg3)

#Racial Demographic Distribution
ggplot(reg3, aes(x = percent, fill = race)) +
  geom_density() +
  facet_grid(race ~ .) +
  labs(title="Distributions of Students by Race")
```

Most NY schools have a low White and Asian population, but a high URM population. Now where are they located?

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}


toBeRemoved<-which(reg3$race=="pctBlackHispanic")
reg4 <-reg3[-toBeRemoved,]

ggplot(reg4, aes( x = as.factor(district), fill = race)) + 
  geom_bar() + 
  labs(title="Racial Breakdown per District", 
       x= "District",
       y = "Number of Students")

```
From our Districts, we can see that most of our students come from Districts 9, 10, 31, 2, and 27.

```{r echo=F, message=FALSE, warning=FALSE, paged.print=TRUE, error= T}
#Heatmap of Race
map <- get_map(location = "New York City", zoom= 11, maptype = "terrain", source="google")
ggmap(map) +
  stat_summary_2d(geom = "tile",bins = 50, data= reg3, aes(x = long, y = lat, z = percent), alpha=0.5) +
  scale_fill_gradient(low = "green", high = "red", guide = guide_legend(title = "Density")) +
  labs(x = "", y= "")
```
From the looks of our graphs, schools are racially stratified within NYC. Asian and White students are relatively spread out, but it is clear that they are in more affluent areas of the city (e.g. Manhattan, Staten Island). Interestingly, URMs are racially stratified in where they attend school, with Hispanics mostly in the northern parts of the Bronx, Brooklyn and Queens while Black students are in the heart of Brooklyn and some in Harlem. Now, when we combine our URMs, we can see three clear clusters of where they primarily attend school: the Bronx, Brooklyn-Queens, and a new cluster we did not see before, West New Brighton.

From this exploratory plot, we're seeing that race and income interact with each other in the form of stratification, so let's take further look at the ENI of NYC:

```{r echo=F, message=FALSE, warning=FALSE, paged.print=TRUE, error= T}
#Heatmap of ENI
ggmap(map) +
  stat_summary_2d(geom = "tile",bins = 15, data=reg3, aes(x = long, y = lat, z = eni), alpha=0.5) +
  scale_fill_gradient(low = "green", high = "red", guide = guide_legend(title = "ENI")) +
  labs(x = "", y= "")
```

As suspected, our schools with a high URM have a high ENI. Let's further explore:

```{r echo=F, message=FALSE, warning=FALSE, paged.print=TRUE, }
ggplot(reg3, aes(x = eni, y = percent, col = race)) +
  geom_point() +
  facet_wrap(.~race)

raceENI <- lm(eni~pctBlackHispanic, data = school_clean)
summary(raceENI)
```
As we can see, there is a strong correlation between URM and ENI, and White students and ENI. Whereas the former is positively correlated, the latter is negatively correlated. This, in addition to the .6 correlation beweetn URM and ENI, tells us that we can predict which schools will have a higher economic need if they have a higher URM population.


## Academic Performance

Now let's take a look at academic performance as it is the main thing we're looking to improve for NYCPASS:

```{r echo=F, message=FALSE, warning=FALSE, paged.print=TRUE}
ggplot(school_clean, aes(eni, academicScore)) +
  geom_point() + 
  stat_smooth(method = "lm") +
  labs(title="The more in need the school, the lower the average scores.", 
       x= "ENI",
       y="Aggregate ELA and Math Scores")
scoreENImodel <- lm(academicScore ~ eni + pctBlackHispanic, data = school_clean)
summary(scoreENImodel)
```

Economic need is a strong predictor of how well a school will do and it makes sense-- the higher the need students have, the lower the scores will be due to resource insecurity. As we've found out from before, ENI and race have a relatively strong correlation, but there must be schools that despite their high ENI, produce a high number of 4s.



##Examining Academic Performance
```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

reg2 <- gather(school_clean, race, count, c(elaBlack, elaHispanic, mathBlack, mathHispanic), factor_key=TRUE)
reg2 <- reg2 %>%
  filter(!is.na(count) & count>0)

reg2$countScale<- scale(reg2$count)

school_16<- reg2 %>%
  group_by(school)%>%
  filter(countScale>1)%>%
  summarise(count = sum(count))


#Racial Demographic Distribution
ggplot(reg2, aes(y = count, x = eni, col = race)) +
  geom_point() +
  facet_grid(. ~ race) +
  labs(title="Distributions of Highest Scores by Students and Race")

summary(school_clean)
```


One way of understanding performance is examining the number of URM students receiving 4s for their exams, but not just any schools, why not target the highest schools? We target schools who fall 1 standard deviation above the mean (top 16% of schools) to produce a list of schools we can specifically target with NYCPASS's programs to further bolster their numbers:

```{r}
school_16 <- school_16 %>%
  arrange(desc(count))
school_16
```
### Recommendation 1
Just as recruiters for companies have "target schools" from which they heavily recruit their college graduate workforce, NYCPASS can have target schools where they would want to implement/pilot their programs, and focus their financial and human resources. The schools above represent the the top 16% schools with URMs that receive 4s (the highest score) on the SHSAT.


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, error= T}
ggmap(map) +
  stat_summary_2d (geom = "tile", bins = 10, data = reg2, aes(x = long, y = lat, z = count), alpha = 0.6) +
  scale_fill_gradient(low = "black", high = "green", guide = guide_legend(title = "Density of 4 Scores")) +
  labs( x="",
        y ="")
ggplot(reg2, aes(x=reorder(City,count, length))) +
  geom_bar(fill = "orange") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggplot(school_clean, aes(pctHispanic, avgMath)) +
  geom_point(col = ifelse(school_clean$avgMath > 3.0 & school_clean$pctHispanic>.50, "red", "black"))

Hispanic <- school_clean%>%
  filter(academicScore >=6.0 & pctHispanic >=.5)

ggplot(school_clean, aes(pctBlackHispanic, academicScore)) +
  geom_point(col = ifelse(school_clean$academicScore > 6.0 & school_clean$pctBlackHispanic>.50, "red", "black"))


Black <- school_clean%>%
  filter(academicScore >=6.0 & pctBlack >=.5)


hipo <- bind_rows(Hispanic, Black)
hipo

ggplot(hipo, aes(as.factor(City), fill = City)) +
  geom_bar(stat = "count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  guides(fill=F) +
   labs(title="What Areas have the highest diversity and average scores?", 
       x= "City",
       y="Academic Score")
```

### Recommendation 2: Geolocation Strategy
Based on the information above, we can see there is a clear corridor of high scores in high need, URM schools. PASSNYC can concentrate marketing and outreach efforts to this area to begin raising awareness for their solutions so to gain the buy-in from parents, teachers, and administrators. In addition, based on the table above, should NYCPASS see a demographic underrrepresented in SHSAT numbers in the future, they can also refer to the `hipo` to identify schools with top performing Black students, or top performing Hispanic students.


## Correlation Matrix
```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
numeric<- na.omit(school_clean)

numeric

school_clean%>%
  select(eni, income, URM4, contains("pct"), contains('Black'), starts_with('avg'), starts_with('quant'))%>%
  cor(use = "pairwise.complete.obs") %>%
  corrplot(type = "upper", method = "square") 

```

###Machine Learning

Let's start with clustering. We can only use District 5 data, since they are the only schools reporting number of students who took the SHSAT. Note: this is such a small amount of data, we might have that our clusters have weak, or no structure but let's try it out.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
num.school <- school_clean%>%
  filter(!is.na(took))%>%
  select(-income, -City, -address, -DBN, -registered, -yield, -took, -enroll,-regPct, -commSchool, -lat, -long, -zip, -district, -elaAsian, -mathAsian, -mathWhite, -pctELAAsian, -elaAsian, -pctMathAsian, -pctMathWhite, -(contains("rating")))

scaled.school<- (scale(num.school[,2:43]))


fviz_nbclust(scaled.school, kmeans, method = "wss")
fviz_nbclust(scaled.school, kmeans, method = "silhouette")
final <- kmeans(scaled.school, 3, nstart = 25)
print(final)
fviz_cluster(final, data = scaled.school)

numsum<- num.school %>%
  mutate(Cluster = final$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")

str(numsum)
```

So based on our elbow and silhouette plots, we should either use 2 or 3 clusters, but based on our silhouette plot, the reality is with either choice, the low number of data points gives us clusters with weak to non-existent structure. Nonetheless, the analysis above does render some interesting information.

For the purpose of our analysis, we used 3 clusters (because there's an interesting outlier as we can see), which explains about 40% of our variability. It's not too bad given our limited data. So what did we find?

* Cluster 2 (Well Off Cluster)is distinct because it has a much lower ENI (wealthier), larger Asian and White population, no one learning ELL, higher attendance rates, higher test scores, higher academic scores, and perfect psychosocial ratings.

* Cluster 3 (Diverse Cluster) is distinct because it has the highest ENI of the three groups, lowest leadership and trust scores, lowest psychosocial ratings, lowest test scores, lowest number of 4s, and highest number of community schools.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

num.school %>%
  mutate(Cluster = final$cluster)%>%
  filter(Cluster ==1)

```

### Recommendation 2.5

*Cluster 1 (Hi-Potential Cluster) is distinct because it has the highest number of URMs, lowest attendance rates, highest math scores for URMs, and most number of URM 4 scores. These schools represent a high diversity, high performance demographic as in many ways it replicates the performance of cluster 2, but the diversity and social needs of Cluster 3. While out of scope for this project, one recommendation for NYPASS is to find schools with similar characteristics of this school and invest their time and resources since it represents a school with high potential for increasing diversity in the SHSATs.


Now we will build a linear regression model that will predict turnout based on variables with high correlations, but first we will need to remove high correlation variables to prevent overfitting.
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
#Linear model to predict the SHSAT test rates for other districts
correlation = cor(scaled.school)
hc = findCorrelation(correlation, cutoff=0.75)
hc = sort(hc)
reduced_Data = num.school[,-c(hc)]
print (reduced_Data)


corrplot(cor(reduced_Data[2:21]), type = "upper", method = "square")

```

Based on the feature selection above, it looks like our optimal model to predict the number of students who take the SHSAT depends on community strength, attendance, and the number of ELA 4 scores schools receive. How interesting.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
modelml <- lm(tookPct~ pctCommunity + pctAttend + elaAll ,data = num.school)
summary(modelml)


#new dataset with filled took %
schoolPredict <- school_clean %>%
  mutate(tookPct = predict(modelml, school_clean))%>%
  filter(!is.na(tookPct))%>%
  select(-income, -City, -address, -DBN, -registered, -yield, -took, -enroll,-regPct ,-commSchool, -school, -lat, -long, -zip, -district, -elaAsian, -mathAsian, -mathWhite, -pctELAAsian, -elaAsian, -pctCommunity, -pctAttend, -elaAll, -mathAll, -quantCommunity, -pctMathAsian, -pctMathWhite, -(contains("rating")))
```

Now that we've created our model and predicted the values for the other districts, we will use k-means clustering, Naive Bayes and Random Forests to build models to predict SHSAT turnout.
 

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
#remove nzv
nzv_cols2 <- nearZeroVar(schoolPredict)
if(length(nzv_cols2) > 0) schoolPredict <- schoolPredict[, -nzv_cols2]

#elbow and silhouette
schoolPredictClust<- schoolPredict%>% select(-tookPct)
fviz_nbclust(schoolPredictClust, kmeans, method = "wss")
fviz_nbclust(schoolPredictClust, kmeans, method = "silhouette")
final2 <- kmeans(schoolPredictClust, 2, nstart = 25)
print(final2)
fviz_cluster(final2, data = schoolPredictClust)

#clustering
schoolPredictSum<- schoolPredictClust%>%
  mutate(Cluster = final2$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")

str(schoolPredictSum)
```

Cluster 1 (Well-Off) has a lower ENI, larger white and Asian population, lower chronic absenteeism, substantially more 4s and URM 4 scores despite there being less of them in the district.

Clust 2 (High Need) has a higher ENI, about double the URM population and absentee rates of the Well-Off cluster, very low number of 4 scores, and many community schools.

The cluster analysis above sets k=2 which explains for about 52.9% of our variability which is not bad at all.


```{r message=FALSE, warning=FALSE, paged.print=FALSE}

#include clusters as features
schoolPredict <- schoolPredict %>%
  mutate(Cluster = final2$cluster)

### Random Forest

schoolPredict$tookFactor <- ifelse(schoolPredict$tookPct < .15, 'low', 'high')
schoolPredict$tookFactor[schoolPredict$tookPct >= .16 & schoolPredict$tookPct <= .30] <- 'okay'
schoolPredict$tookFactor <- factor(schoolPredict$tookFactor, levels = c("low", "okay", "high"), ordered = T)
summary(schoolPredict$tookFactor)

set.seed(123)

trainIndex <- createDataPartition((schoolPredict$tookFactor), p = .6, 
                                  list = FALSE, 
                                  times = 1)

train <- schoolPredict[ trainIndex,]
test  <- schoolPredict[-trainIndex,]

ValidationIndex <- createDataPartition((test$tookFactor), p = .6, 
                                  list = FALSE, 
                                  times = 1)

validation <- test[ ValidationIndex,]
test  <- test[-ValidationIndex,]


# prepare training scheme
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
nbmodel <- train(tookFactor~.-tookPct, data=train, trControl=train_control, method="nb")
# summarize results
print(nbmodel)

#Bayes gives us a 73%% accuracy.
pred1<-predict(nbmodel, newdata = validation)
confusionMatrix(pred1, validation$tookFactor)


#Random Forest gives us a 81.7% accuracy.
rfmodel <- randomForest(tookFactor ~ . -tookPct, data = train, importance = T)
pred <- predict(rfmodel, newdata = validation)
confusionMatrix(pred, validation$tookFactor)

rfmodel
# estimate variable importance
importance <- varImp(rfmodel)
# summarize importance
print(importance)
# plot importance
varImpPlot(rfmodel, type = 2)

#We are selecting Random Forest model as our final model. We get 81.8% accuracy.
pred2 <- predict(rfmodel, newdata = test)
confusionMatrix(pred2, test$tookFactor)

```





